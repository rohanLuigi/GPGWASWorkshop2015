{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leave-out-one Cross Validation\n",
    "\n",
    "### November 2015\n",
    "### Rohan Fernando, Emre Karaman, Hao Cheng and Dorian Garrick"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Training is based on leaving out observation $j$ from the training data, for $j=1,\\ldots, n$ \n",
    "* Then, $y_j$ is predicted as $\\hat{y}_j = \\mathbf{x}_j'\\hat{\\beta}$, where $\\hat{\\beta}$ is the estimate of $\\beta$ estimated from the data set where observation $j$ was left out. \n",
    "* Thus, cross validation by straightforward application of this approach, would require $n$ analyses with $n-1$ observations in each analysis. \n",
    "* Fortunately, when the marker effects model (MEM) is used, leave-one-out cross validation can be performed with little more effort than is required for a single analysis with $n$ observations by using a well-known strategy used in least-squares regression to compute the predicted residual sum of squares (PRESS) statistic. \n",
    "* When $k>n$ the breeding value model (BVM) is more efficient because for this model the mixed model equations are of order $n\\times n$. \n",
    "* We show below how leave-one-out cross validation can also be performed using either or the MEM and BVM with little more effort than is required for a single analysis with $n$ observations. Dan Gianola is also working on this problem, even for the Bayesian alphabet methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Marker Effects Model\n",
    "Use of the MEM is more efficient when $n>k$ because for this model the mixed model equations are of order $k\\times k$. The MEM for G-BLUP can be written as\n",
    "\n",
    "$$\n",
    "        \\mathbf{y}=\\mathbf{X}\\mathbf{\\beta}+\\mathbf{e},\n",
    "$$\n",
    "\n",
    "where $\\mathbf{y}$ has been corrected for all effects other than $\\mathbf{\\beta}$, the marker effects, and $\\mathbf{X}$ is the matrix of marker covariates. Often it is assumed that marker effects are identically and independetly distributed (iid) random variables with null means and variances $\\sigma^2_\\beta$. Thus, under the usual assumption that the resudual are iid with null means and variances $\\sigma^2_e$, $\\text{E}(\\mathbf{y}) = \\mathbf{0}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, BLP of $\\beta$\n",
    "can be obtained by solving the $k\\times k$ mixed model equations\n",
    "\n",
    "$$\n",
    "\\left(\\mathbf{X'X}+\\mathbf{I}\\lambda\\right)\\hat{\\mathbf{\\beta}}=\\mathbf{X'y},\n",
    "$$\n",
    "where $\\lambda=\\frac{\\sigma_{e}^{2}}{\\sigma_{\\beta}^{2}}$.\n",
    "\n",
    "Now, BLUP for $\\mathbf{\\beta_{-j}}$, where observation $j$ is\n",
    "left out, can be obtained as\n",
    "\n",
    "$$\n",
    "\\mathbf{\\hat{\\beta}}_{-j}=\n",
    "\\left(\\mathbf{X}_{-j}'\\mathbf{X}_{-j}+\\mathbf{I}\\lambda\\right)^{-1}\\mathbf{X}_{-j}'\n",
    "    \\mathbf{y}_{-j},\\label{eq:PRESS1-1} \\tag{1}\n",
    "$$\n",
    "\n",
    "where $\\mathbf{X}_{-j}$ is $\\mathbf{X}$ with the $j$th\n",
    "row removed and $\\mathbf{y}_{-j}$ is $\\mathbf{y}$ with the\n",
    "$j$th element removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But, from the matrix inverse lemma,\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\left(\\mathbf{X}_{-j}'\\mathbf{X}_{-j}+\\mathbf{I}\\lambda\\right)^{-1} & =\\left(\\mathbf{X}'\\mathbf{X}+\\mathbf{I}\\lambda-\\mathbf{x}_{j}\\mathbf{x}_{j}^{'}\\right)^{-1}\\nonumber \\\\\n",
    " & =\\left(\\mathbf{X}'\\mathbf{X}+\\mathbf{I}\\lambda\\right)^{-1}-\\frac{\\left(\\mathbf{X}'\\mathbf{X}+\\mathbf{I}\\lambda\\right)^{-1}\\mathbf{x}_{j}\\mathbf{x}_{j}^{'}\\left(\\mathbf{X}'\\mathbf{X}+\\mathbf{I}\\lambda\\right)^{-1}}{1-H_{jj}},\\label{eq:PRESS2-1} \\tag{2}\n",
    "\\end{align} \n",
    "$$\n",
    "\n",
    "where the quadratic $H_{jj}=\\mathbf{x}_{j}^{'}\\left(\\mathbf{X}'\\mathbf{X}+\\mathbf{I}\\lambda\\right)^{-1}\\mathbf{x}_{j}$\n",
    "is the $j$th diagonal element of $\\mathbf{H}=\\mathbf{X}\\left(\\mathbf{X}'\\mathbf{X}+\\mathbf{I}\\lambda\\right)^{-1}\\mathbf{X}^{'}$.\n",
    "\n",
    "Using (\\ref{eq:PRESS2-1}) in (\\ref{eq:PRESS1-1}), the prediction\n",
    "residual for the $j$th observation can be written as "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "\\hat{e_{j}} & =y_{j}-\\mathbf{x}_{j}^{'}\\hat{\\mathbf{\\beta}}_{-j}\\\\\n",
    " & =y_{j}-\\mathbf{x}_{j}^{'}\\left[\\left(\\mathbf{X}'\\mathbf{X}+\\mathbf{I}\\lambda\\right)^{-1}-\\frac{\\left(\\mathbf{X}'\\mathbf{X}+\\mathbf{I}\\lambda\\right)^{-1}\\mathbf{x}_{j}\\mathbf{x}_{j}^{'}\\left(\\mathbf{X}'\\mathbf{X}+\\mathbf{I}\\lambda\\right)^{-1}}{1-H_{jj}}\\right]\\mathbf{X}_{-j}'\\mathbf{y}_{-j}\\\\\n",
    " & =\\frac{y_{j}-\\mathbf{x}_{j}^{'}\\hat{\\mathbf{\\beta}}}{1-H_{jj}}.\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then PRESS is calculated as $\\sum_{j=1}^{n}\\hat{e_{j}}^{2}$. \n",
    "The accuracy\n",
    "of genomic prediction is often quantified as the correlation between\n",
    "the predicted and observed values of $y_{j}$, and this correlation\n",
    "can be estimated from the values of $\\hat{y}_{j}$ efficiently computed\n",
    "as $\\hat{y}_{j}=y_{j}-\\hat{e}_{j}$ and the observed values of $y_{j}.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Breeding Value Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When $n < k$, $\\mathbf{x}_{j}'\\hat{\\mathbf{\\beta}}$ can be obtained more efficiently by solving the mixed model equations (MME) for the BVM: \n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbf{y} & =\\mathbf{Zu}+\\mathbf{e},\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where $\\mathbf{u}=\\mathbf{X}\\mathbf{\\beta}$,  $\\text{Var}\\left(\\mathbf{u}\\right)=\\mathbf{XX'}\\sigma_{\\beta}^{2}$, and $\\mathbf{Z}$ is the identity matrix of order $k$. As in the MEM, in this model also $\\text{E}(\\mathbf{y}) = \\mathbf{0}$, and in both models $\\text{Var}(\\mathbf{y}) = \\mathbf{X}\\mathbf{X}'\\sigma_{\\beta}^{2}+\\mathbf{I}\\sigma_{e}^{2}$. Thus, these two models are said to be equivalent, and yield identical pedictions.\n",
    "\n",
    "The MME for this model are:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\left(\\mathbf{Z'Z}+\\mathbf{G}^{-1}\\lambda\\right)\\hat{\\mathbf{u}}=\\mathbf{Z'y},\n",
    "$$\n",
    "\n",
    "where $\\mathbf{G} = \\mathbf{XX'}$. So, because of the equivalence of these two models, when $n < k$, $\\mathbf{x}_{j}'\\hat{\\mathbf{\\beta}}$ is more efficiently obtained as $\\hat{u}_j$. Similarly, \n",
    "$\\text{Var}(\\mathbf{x}_{j}'\\mathbf{\\beta} - \\mathbf{x}_{j}'\\hat{\\mathbf{\\beta}}) = \n",
    "\\mathbf{x}_{j}'\\left(\\mathbf{X}'\\mathbf{X}+\\mathbf{I}\\lambda\\right)^{-1}\\mathbf{x}_{j}\\sigma_{e}^{2}$ can be obtained more efficiently as \n",
    "$\\text{Var}(u_j - \\hat{u}_j) = \n",
    "\\mathbf{z}_{j}'\\left(\\mathbf{Z'Z}+\\mathbf{G}^{-1}\\lambda\\right)^{-1}\\mathbf{z}_{j}\\sigma_{e}^{2}$. Using these two equivalent results, the formula for $\\hat{e}_j$ becomes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "\\hat{e_{j}} & =y_{j}-\\mathbf{x}_{j}^{'}\\hat{\\mathbf{\\beta}}_{-j}\\\\\n",
    " & =\\frac{y_{j}-\\mathbf{x}_{j}^{'}\\hat{\\mathbf{\\beta}}}{1-H_{jj}}\\\\\n",
    " & =\\frac{y_{j}-\\mathbf{x}_{j}^{'}\\hat{\\mathbf{\\beta}}}{1-\\mathbf{x}_{j}'\\left(\\mathbf{X}'\\mathbf{X}+\\mathbf{I}\\lambda\\right)^{-1}\\mathbf{x}_{j}}\\\\\n",
    " & =\\frac{y_{j} - \\hat{u}_j}\n",
    " {1-\\mathbf{z}_{j}'\\left(\\mathbf{Z}'\\mathbf{Z}+\\mathbf{G}^{-1}\\lambda\\right)^{-1}\\mathbf{z}_{j}}\\\\\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, $\\mathbf{Z}$ is the identity matrix, and so \n",
    "$\\mathbf{z}_{j}'\\left(\\mathbf{Z}'\\mathbf{Z}+\\mathbf{G}^{-1}\\lambda\\right)^{-1}\\mathbf{z}_{j}$ becomes $c^{j,j}$, \n",
    "the $j$th diagonal of the inverse of $\\mathbf{C} = \\left(\\mathbf{Z}'\\mathbf{Z}+\\mathbf{G}^{-1}\\lambda\\right)$.  Then,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "\\hat{e_{j}} &  =\\frac{y_{j} - \\hat{u}_j}\n",
    " {1-c^{j,j}}.\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "simDat (generic function with 1 method)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Distributions\n",
    "function simDat(nObs,nLoci,bMean,bStd,resStd)\n",
    "    X = sample([0;1;2],(nObs,nLoci))\n",
    "    b = rand(Normal(bMean,bStd),size(X,2))\n",
    "    y = X*b + rand(Normal(0.0, resStd),nObs)\n",
    "    return (y,X,b)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "srand(31415) # seed for random number generator\n",
    "vara = 50\n",
    "vare = 200\n",
    "nObs,nLoci,bMean,bStd,resStd = 5,10,0.0,sqrt(vara),sqrt(vare)\n",
    "res = simDat(nObs,nLoci,bMean,bStd,resStd);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5x10 Array{Int64,2}:\n",
       " 2  0  2  0  0  1  0  0  0  2\n",
       " 2  0  0  2  1  1  1  1  0  1\n",
       " 0  0  1  0  1  0  0  0  1  0\n",
       " 2  1  0  2  2  1  0  0  1  0\n",
       " 2  2  2  1  0  2  0  0  0  1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = res[1]\n",
    "X = res[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inefficient Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LOjEHat (generic function with 1 method)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function LOjEHat(X,y,vara,vare,j)\n",
    "    λ = vare/vara\n",
    "    n = size(X,1)\n",
    "    k = size(X,2)\n",
    "    xPj = X[j,:]\n",
    "    yj  = y[j]\n",
    "    sel = fill(true,n)\n",
    "    sel[j] = false\n",
    "    X = X[sel,:]\n",
    "    y = y[sel]\n",
    "    betaHat = inv(X'X + eye(k)*λ)*X'y\n",
    "    return yj - xPj*betaHat\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5-element Array{Any,1}:\n",
       " [-27.273554798523264]\n",
       " [-10.482056545905907]\n",
       " [-9.263847750012747] \n",
       " [-3.881854751591085] \n",
       " [57.63494553766701]  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eHatN = [LOjEHat(X,y,vara,vare,j) for j=1:nObs];\n",
    "eHatN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1-element Array{Float64,1}:\n",
       " 4276.39"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eHatN'eHatN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Efficient Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results from HMME for MEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5x5 Array{Float64,2}:\n",
       "  0.587565    0.101121    0.074395   -0.0720657   0.209591 \n",
       "  0.101121    0.571702   -0.0688731   0.217864    0.0272882\n",
       "  0.074395   -0.0688731   0.366327    0.126923   -0.0124895\n",
       " -0.0720657   0.217864    0.126923    0.598508    0.12114  \n",
       "  0.209591    0.0272882  -0.0124895   0.12114     0.638768 "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "λ = vare/vara\n",
    "H = X*inv(X'X + eye(nLoci)*λ)*X'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5-element Array{Float64,1}:\n",
       " -27.2736 \n",
       " -10.4821 \n",
       "  -9.26385\n",
       "  -3.88185\n",
       "  57.6349 "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = 1 - diag(H)\n",
    "eHatMEM = (y - H*y) ./ d "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1-element Array{Float64,1}:\n",
       " 4276.39"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eHatMEM'eHatMEM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy of prediction: cor($y$,$\\hat{y}$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5-element Array{Float64,1}:\n",
       " 29.1861 \n",
       " 12.8075 \n",
       "  1.48517\n",
       " 17.7184 \n",
       "  6.19443"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yHat = y - eHatMEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.2326421820030729"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cor(y,yHat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Same Results from HMME for BVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5x5 Array{Float64,2}:\n",
       "  0.372072  -0.23288   -0.27147    0.267901  -0.250279\n",
       " -0.23288    0.390364   0.292215  -0.347535   0.120678\n",
       " -0.27147    0.292215   0.675887  -0.368726   0.164622\n",
       "  0.267901  -0.347535  -0.368726   0.447691  -0.212581\n",
       " -0.250279   0.120678   0.164622  -0.212581   0.261878"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z    = eye(5)\n",
    "Gi   = inv(X*X')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5x5 Array{Float64,2}:\n",
       "  0.587565    0.101121    0.074395   -0.0720657   0.209591 \n",
       "  0.101121    0.571702   -0.0688731   0.217864    0.0272882\n",
       "  0.074395   -0.0688731   0.366327    0.126923   -0.0124895\n",
       " -0.0720657   0.217864    0.126923    0.598508    0.12114  \n",
       "  0.209591    0.0272882  -0.0124895   0.12114     0.638768 "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Lhs = Z'Z + Gi*λ\n",
    "Rhs = Z'y\n",
    "iLhs = inv(Lhs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5-element Array{Float64,1}:\n",
       " 13.1611 \n",
       "  6.81487\n",
       " -1.90843\n",
       " 15.3951 \n",
       " 43.0098 "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sol  = iLhs*Rhs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5-element Array{Float64,1}:\n",
       " 0.412435\n",
       " 0.428298\n",
       " 0.633673\n",
       " 0.401492\n",
       " 0.361232"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = 1 - diag(Z*iLhs*Z')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5-element Array{Float64,1}:\n",
       " -27.2736 \n",
       " -10.4821 \n",
       "  -9.26385\n",
       "  -3.88185\n",
       "  57.6349 "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eHatBVM = (y - Z*sol)./d "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy of prediction: cor($y$,$\\hat{y}$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5-element Array{Float64,1}:\n",
       " 29.1861 \n",
       " 12.8075 \n",
       "  1.48517\n",
       " 17.7184 \n",
       "  6.19443"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yHat = y - eHatBVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.23264218200307246"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cor(y,yHat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5x2 Array{Float64,2}:\n",
       " -27.2736   -27.2736 \n",
       " -10.4821   -10.4821 \n",
       "  -9.26385   -9.26385\n",
       "  -3.88185   -3.88185\n",
       "  57.6349    57.6349 "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[eHatMEM  eHatBVM]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MCMC Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Gibbs (generic function with 1 method)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function Gibbs(A,x,b)\n",
    "    n = size(x,1)\n",
    "    for i=1:n\n",
    "        cVar = 1.0/A[i,i]\n",
    "        cMean   = cVar*(b[i] - A[:,i]'x)[1] + x[i]\n",
    "        x[i]    = randn()*sqrt(cVar*vare) + cMean \n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nIter = 1000000\n",
    "k = size(Lhs,1)\n",
    "b = zeros(k)\n",
    "ss = zeros(nIter,k)\n",
    "for i =1:nIter\n",
    "    Gibbs(Lhs,b,Rhs)\n",
    "    ss[i,:] = b'\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5x1 Array{Float64,2}:\n",
       " 13.1799 \n",
       "  6.80966\n",
       " -1.89962\n",
       " 15.3909 \n",
       " 43.0179 "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bMCMC = mean(ss,1)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5x1 Array{Float64,2}:\n",
       " 0.412045\n",
       " 0.426936\n",
       " 0.633042\n",
       " 0.402129\n",
       " 0.36139 "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dMCMC = 1 - var(ss,1)'/vare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5x1 Array{Float64,2}:\n",
       " -27.319  \n",
       " -10.4699 \n",
       "  -9.27776\n",
       "  -3.87131\n",
       "  57.6123 "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eHatMCMC = (y - Z*bMCMC)./ d "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of Results from MEM, BVM, BVM-MCMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5x3 Array{Float64,2}:\n",
       " -27.2736   -27.2736   -27.319  \n",
       " -10.4821   -10.4821   -10.4699 \n",
       "  -9.26385   -9.26385   -9.27776\n",
       "  -3.88185   -3.88185   -3.87131\n",
       "  57.6349    57.6349    57.6123 "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[eHatMEM  eHatBVM eHatMCMC]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1x3 Array{Float64,2}:\n",
       " 4276.39  4276.39  4276.19"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[eHatMEM'eHatMEM eHatBVM'eHatBVM eHatMCMC'eHatMCMC]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Breeding value model with $n$=1000 and $k$=50,000\n",
    "#### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "        \\mathbf{y}=\\mathbf{X}\\mathbf{\\beta} + \\mathbf{Zu} + \\mathbf{e},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $\\mathbf{u} = \\mathbf{M\\alpha}$, $\\mathbf{M}$ is the matrix of marker covariates and $\\mathbf{\\alpha}$ are the marker effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "simDat (generic function with 1 method)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Distributions\n",
    "using Gadfly\n",
    "function simDat(nObs,nLoci,bMean,bStd,resStd)\n",
    "    X=[ones(nObs,1) sample([0.0,1.0,2.0],(nObs,nLoci))]\n",
    "    b=[100.0; rand(Normal(bMean,bStd),nLoci)]\n",
    "    y=X*b+rand(Normal(0.0,resStd),nObs)\n",
    "return(y,X,b)\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14878684486281213"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "srand(1234)\n",
    "nObs   = 1000;\n",
    "nLoci  = 50000;\n",
    "bMean  = 0.0;\n",
    "bStd   = sqrt(1.0/(nLoci*0.5))\n",
    "resStd = 1.0\n",
    "res    = simDat(nObs,nLoci,bMean,bStd,resStd)\n",
    "vare   = resStd^2\n",
    "varb   = bStd^2\n",
    "y = res[1]\n",
    "X = res[2]\n",
    "b = res[3]\n",
    "M = X[:,2:end]\n",
    "X = X[:,1]\n",
    "λ = vare/varb\n",
    "\n",
    "Gi   = inv(M*M')\n",
    "Z    = eye(nObs);\n",
    "Lhs  = [X'X X'Z\n",
    "        Z'X (Z'Z+Gi*λ)]\n",
    "Rhs  =  [X'y \n",
    "        Z'y]\n",
    "iLhs = inv(Lhs);\n",
    "W    = [X Z]\n",
    "sol  = iLhs*Rhs\n",
    "d    = 1 - diag(W*iLhs*W')\n",
    "eHatBVM = (y - W*sol)./d \n",
    "yHat    = y - eHatBVM\n",
    "cor(y,yHat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9981111316697726"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cor(y,W*sol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100.61705265784505"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yHat[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "1. Compute $\\hat{y}_1$ by training with a dataset leaves out $y_1$. Compare result with that obtained above.\n",
    "2. Why is the accuracy so low?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.4.0",
   "language": "julia",
   "name": "julia-0.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
